<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evals-Are-All-You-Need</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f0f4f8;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 320px;
            max-height: 400px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 384px;
            }
        }
        .stat-card h3 {
            color: #0044cc;
        }
        .flowchart-step {
            border-color: #3399ff;
            background-color: #e6f2ff;
        }
        .flowchart-arrow {
            color: #0077ff;
        }
    </style>
</head>
<body class="text-gray-800">

    <div class="container mx-auto p-4 md:p-8">

        <header class="text-center mb-12">
            <h1 class="text-4xl md:text-5xl font-extrabold text-[#0044cc] mb-4">The Benchmark Paradox</h1>
            <p class="text-lg md:text-xl text-gray-600 max-w-3xl mx-auto">AI models are achieving superhuman scores on complex benchmarks, yet a critical gap persists between this measured "skill" and the "intelligence" required for genuine scientific discovery.</p>
        </header>

        <section id="performance-growth" class="mb-16">
            <div class="bg-white rounded-lg shadow-lg p-6 md:p-8">
                <h2 class="text-3xl font-bold text-center mb-2">An Explosion in Capability</h2>
                <p class="text-center text-gray-600 mb-8">Performance on demanding, general-purpose benchmarks has skyrocketed in the last year, signaling rapid advancements in AI's problem-solving abilities.</p>
                <div class="chart-container">
                    <canvas id="benchmarkGrowthChart"></canvas>
                </div>
            </div>
        </section>

        <section id="reality-check" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-8">The Reality Check: Skill vs. Scientific Intelligence</h2>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold text-center mb-4 text-[#0044cc]">What Benchmarks Measure: Textbook Q&A</h3>
                    <p class="text-gray-600 mb-6 text-center">Most benchmarks evaluate a model's ability to solve fixed, well-defined problems, similar to an open-book exam.</p>
                    <div class="space-y-4">
                        <div class="flowchart-step text-center p-3 rounded-lg border-2 border-dashed">Input: Static Problem</div>
                        <div class="text-center text-4xl flowchart-arrow font-light">&darr;</div>
                        <div class="flowchart-step text-center p-3 rounded-lg border-2 border-dashed">Process: Retrieve & Apply Known Information</div>
                        <div class="text-center text-4xl flowchart-arrow font-light">&darr;</div>
                        <div class="flowchart-step text-center p-3 rounded-lg border-2 border-dashed">Output: A Single, Correct Answer</div>
                    </div>
                </div>
                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold text-center mb-4 text-[#0044cc]">What Science Requires: Iterative Discovery</h3>
                     <p class="text-gray-600 mb-6 text-center">True scientific inquiry is a dynamic, cyclical process of exploration, experimentation, and refinement.</p>
                    <div class="space-y-4">
                        <div class="flowchart-step text-center p-3 rounded-lg border-2 border-dashed">Generate Hypothesis</div>
                        <div class="text-center text-4xl flowchart-arrow font-light">&darr; &uarr;</div>
                        <div class="flowchart-step text-center p-3 rounded-lg border-2 border-dashed">Design & Run Experiment</div>
                        <div class="text-center text-4xl flowchart-arrow font-light">&darr; &uarr;</div>
                        <div class="flowchart-step text-center p-3 rounded-lg border-2 border-dashed">Observe, Analyze & Refine</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="deep-dive" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-8">Domain Deep Dive: Where Benchmarks Succeed and Fail</h2>
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                
                <div class="bg-white rounded-lg shadow-lg p-6 md:p-8">
                    <h3 class="text-2xl font-bold mb-1">🧪 Chemistry: The Illusion of Progress</h3>
                    <p class="text-gray-600 mb-6">Many popular drug discovery benchmarks suffer from data contamination, where test molecules are too similar to training data. This leads to overoptimistic results that don't reflect real-world novelty challenges.</p>
                    <div class="chart-container">
                        <canvas id="chemistryBenchmarkChart"></canvas>
                    </div>
                </div>

                <div class="bg-white rounded-lg shadow-lg p-6 md:p-8">
                    <h3 class="text-2xl font-bold mb-1">🧬 Biology: The Gold Standard</h3>
                    <p class="text-gray-600 mb-6">The CASP experiment in protein folding is a model for success. Its community-wide, blind-test format created a rigorous environment that directly led to the AlphaFold breakthrough, accelerating the entire field.</p>
                    <div class="relative mt-8">
                        <div class="border-l-4 border-[#3399ff] absolute h-full top-0 left-4"></div>
                        <div class="mb-8 pl-12 relative">
                            <div class="absolute w-6 h-6 bg-[#0077ff] rounded-full -left-3 border-4 border-white"></div>
                            <p class="font-bold text-[#0044cc]">1994: CASP Founded</p>
                            <p class="text-gray-600">Established as a biennial blind experiment to assess protein structure prediction.</p>
                        </div>
                        <div class="mb-8 pl-12 relative">
                             <div class="absolute w-6 h-6 bg-[#0077ff] rounded-full -left-3 border-4 border-white"></div>
                            <p class="font-bold text-[#0044cc]">2020: The AlphaFold Breakthrough</p>
                            <p class="text-gray-600">At CASP14, DeepMind's AlphaFold achieves accuracy competitive with experimental methods.</p>
                        </div>
                        <div class="pl-12 relative">
                             <div class="absolute w-6 h-6 bg-[#0077ff] rounded-full -left-3 border-4 border-white"></div>
                            <p class="font-bold text-[#0044cc]">2022: A New Era</p>
                            <p class="text-gray-600">At CASP15, nearly all top teams use AlphaFold, demonstrating how a good benchmark can catalyze progress.</p>
                        </div>
                    </div>
                </div>

                <div class="bg-white rounded-lg shadow-lg p-6 md:p-8 lg:col-span-2">
                    <h3 class="text-2xl font-bold mb-1">🧮 Math & Physics: Evolving Towards True Reasoning</h3>
                    <p class="text-gray-600 mb-6">Benchmarks in formal mathematics and theoretical physics are moving away from testing rote problem-solving towards evaluating an AI's capacity for genuine, unscripted reasoning on novel, research-level questions.</p>
                    <div class="chart-container">
                        <canvas id="mathPhysicsBenchmarkChart"></canvas>
                    </div>
                </div>
            </div>
        </section>

        <section id="systemic-flaws" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-8">Systemic Flaws in AI Evaluation</h2>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <div class="stat-card bg-white rounded-lg shadow-lg p-6 text-center">
                    <div class="text-5xl mb-4">💧</div>
                    <h3 class="text-xl font-bold mb-2">Data Contamination</h3>
                    <p class="text-gray-600">Test data is often too similar to training data, rewarding memorization over true generalization and leading to inflated performance metrics.</p>
                </div>
                <div class="stat-card bg-white rounded-lg shadow-lg p-6 text-center">
                    <div class="text-5xl mb-4">⬛</div>
                    <h3 class="text-xl font-bold mb-2">The "Black Box" Problem</h3>
                    <p class="text-gray-600">Models often lack interpretability. In science, a correct answer is not enough; the causal reasoning behind it is paramount for trust and adoption.</p>
                </div>
                <div class="stat-card bg-white rounded-lg shadow-lg p-6 text-center">
                    <div class="text-5xl mb-4">🎯</div>
                    <h3 class="text-xl font-bold mb-2">Misaligned Incentives</h3>
                    <p class="text-gray-600">The race for high scores can lead to "gaming" benchmarks, optimizing for a specific metric rather than for genuine scientific utility.</p>
                </div>
                <div class="stat-card bg-white rounded-lg shadow-lg p-6 text-center">
                    <div class="text-5xl mb-4">🔁</div>
                    <h3 class="text-xl font-bold mb-2">Reproducibility Crisis</h3>
                    <p class="text-gray-600">Complex models and incomplete documentation make reproducing results prohibitively difficult, hindering the creation of new knowledge.</p>
                </div>
            </div>
        </section>

        <footer class="text-center pt-8 border-t border-gray-300">
             <h2 class="text-3xl font-bold text-center mb-4 text-[#0044cc]">The Path Forward</h2>
             <p class="text-lg text-gray-600 max-w-3xl mx-auto mb-8">To unlock AI's potential, we must shift from benchmark-driven leaderboards to problem-driven discovery, fostering AI as a true partner in science.</p>
             <div class="flex flex-col md:flex-row justify-center items-center gap-8">
                <div class="text-center">
                    <p class="text-5xl font-bold text-[#0077ff]">1</p>
                    <p class="font-semibold mt-2">Develop Agentic &<br>Iterative Benchmarks</p>
                </div>
                 <div class="text-center">
                    <p class="text-5xl font-bold text-[#0077ff]">2</p>
                    <p class="font-semibold mt-2">Establish Neutral,<br>Community-Wide Standards</p>
                </div>
                 <div class="text-center">
                    <p class="text-5xl font-bold text-[#0077ff]">3</p>
                    <p class="font-semibold mt-2">Shift to a "Problem-Driven"<br>Development Paradigm</p>
                </div>
             </div>
        </footer>

    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const brilliantBlues = ['#0044cc', '#0077ff', '#3399ff', '#66bbff', '#99ddff'];
            
            const tooltipTitleCallback = function(tooltipItems) {
                const item = tooltipItems[0];
                let label = item.chart.data.labels[item.dataIndex];
                if (Array.isArray(label)) {
                    return label.join(' ');
                } else {
                    return label;
                }
            };

            const wrapLabel = (label) => {
                const maxLen = 16;
                if (label.length <= maxLen) return label;
                const words = label.split(' ');
                const lines = [];
                let currentLine = '';
                for (const word of words) {
                    if ((currentLine + ' ' + word).trim().length > maxLen) {
                        lines.push(currentLine.trim());
                        currentLine = word;
                    } else {
                        currentLine = (currentLine + ' ' + word).trim();
                    }
                }
                if (currentLine) lines.push(currentLine.trim());
                return lines;
            };

            const benchmarkGrowthCtx = document.getElementById('benchmarkGrowthChart').getContext('2d');
            new Chart(benchmarkGrowthCtx, {
                type: 'line',
                data: {
                    labels: ['Q2 2023', 'Q4 2023', 'Q2 2024'],
                    datasets: [{
                        label: 'GPQA Score',
                        data: [35.2, 54.1, 83.0],
                        borderColor: brilliantBlues[0],
                        backgroundColor: brilliantBlues[0] + '33',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'SWE-bench Score',
                        data: [14.1, 48.2, 81.3],
                        borderColor: brilliantBlues[1],
                        backgroundColor: brilliantBlues[1] + '33',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'MMMU Score',
                        data: [34.2, 49.5, 68.3],
                        borderColor: brilliantBlues[2],
                        backgroundColor: brilliantBlues[2] + '33',
                        fill: true,
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: { display: true, text: 'Benchmark Score (%)' }
                        }
                    },
                    plugins: {
                        title: { display: true, text: 'Benchmark Performance Growth Over One Year', font: { size: 16 } },
                        tooltip: { callbacks: { title: tooltipTitleCallback } }
                    }
                }
            });

            const chemistryBenchmarkCtx = document.getElementById('chemistryBenchmarkChart').getContext('2d');
            new Chart(chemistryBenchmarkCtx, {
                type: 'bar',
                data: {
                    labels: ['MoleculeNet (HIV)', 'Lo-Hi (Hit-ID)'],
                    datasets: [{
                        label: 'Reported Performance',
                        data: [82, 75],
                        backgroundColor: brilliantBlues[3],
                    }, {
                        label: 'Realistic Performance',
                        data: [55, 75],
                        backgroundColor: brilliantBlues[1],
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: { display: true, text: 'Model Accuracy (%)' }
                        }
                    },
                    plugins: {
                        title: { display: true, text: 'Unrealistic vs. Practical Benchmark Performance', font: { size: 16 } },
                        tooltip: { callbacks: { title: tooltipTitleCallback } }
                    }
                }
            });

            const mathPhysicsBenchmarkCtx = document.getElementById('mathPhysicsBenchmarkChart').getContext('2d');
            const originalLabels = ['Competition Problems', 'Undergraduate Curriculum', 'Novel Research Questions', 'Iterative Self-Correction', 'Agentic Reasoning'];
            const wrappedLabels = originalLabels.map(wrapLabel);
            new Chart(mathPhysicsBenchmarkCtx, {
                type: 'radar',
                data: {
                    labels: wrappedLabels,
                    datasets: [{
                        label: 'Early Benchmarks (e.g., MiniF2F)',
                        data: [9, 7, 2, 3, 1],
                        borderColor: brilliantBlues[3],
                        backgroundColor: brilliantBlues[3] + '4D',
                    }, {
                        label: 'Modern Benchmarks (e.g., TPBench, SDE)',
                        data: [5, 6, 9, 8, 8],
                        borderColor: brilliantBlues[0],
                        backgroundColor: brilliantBlues[0] + '4D',
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        r: {
                            angleLines: { color: '#ddd' },
                            grid: { color: '#ddd' },
                            pointLabels: { font: { size: 12 } },
                            ticks: { display: false, beginAtZero: true, max: 10 }
                        }
                    },
                    plugins: {
                        title: { display: true, text: 'Shift in Evaluation Focus for Math & Physics', font: { size: 16 } },
                        tooltip: { callbacks: { title: tooltipTitleCallback } }
                    }
                }
            });
        });
    </script>

</body>
</html>
